{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdfe217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasterrisk in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (0.1.5)\n",
      "Requirement already satisfied: pandas==1.5.2 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from fasterrisk) (1.5.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from fasterrisk) (2.28.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.3 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from fasterrisk) (1.23.3)\n",
      "Requirement already satisfied: scikit-learn==1.2.0 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from fasterrisk) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.5.2->fasterrisk) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.5.2->fasterrisk) (2021.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.2.0->fasterrisk) (1.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.2.0->fasterrisk) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.2.0->fasterrisk) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas==1.5.2->fasterrisk) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shafiranaya/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (1.26.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %conda create -n FasterRisk python=3.9 # create a virtual environment\n",
    "# %conda activate FasterRisk # activate the virtual environment\n",
    "%pip install fasterrisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a42a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cba10be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "from fasterrisk.utils import download_file_from_google_drive,  compute_logisticLoss_from_X_y_beta0_betas, get_all_product_booleans, get_support_indices, isEqual_upTo_8decimal, isEqual_upTo_16decimal, get_all_product_booleans\n",
    "\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, log_loss, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f25598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For resampling\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "# from imblearn.over_sampling import SMOTE, SMOTENC, SMOTEN, ADASYN, RandomOverSampler\n",
    "# from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "# from imblearn.combine import SMOTEENN\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Ensemble Classifiers\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV, cross_val_score, train_test_split, KFold, cross_validate\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, log_loss, classification_report, confusion_matrix\n",
    "\n",
    "# from xgboost import plot_importance, to_graphviz\n",
    "\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b61decf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calculation_table(risk_score_model):\n",
    "    assert risk_score_model.featureNames is not None, \"please pass the featureNames to the model by using the function .reset_featureNames(featureNames)\"\n",
    "\n",
    "    nonzero_indices = get_support_indices(risk_score_model.coefficients)\n",
    "\n",
    "    max_feature_length = max([len(featureName) for featureName in risk_score_model.featureNames])\n",
    "    row_score_template = '{0}. {1:>%d}     {2:>2} point(s) | + ...' % (max_feature_length)\n",
    "\n",
    "    print(\"The Risk Score is:\")\n",
    "    for count, feature_i in enumerate(nonzero_indices):\n",
    "        row_score_str = row_score_template.format(count+1, risk_score_model.featureNames[feature_i], int(risk_score_model.coefficients[feature_i]))\n",
    "        if count == 0:\n",
    "            row_score_str = row_score_str.replace(\"+\", \" \")\n",
    "\n",
    "        print(row_score_str)\n",
    "\n",
    "    final_score_str = ' ' * (14+max_feature_length) + 'SCORE | =    '\n",
    "    print(final_score_str)\n",
    "    \n",
    "    \n",
    "    print(\"###\")\n",
    "    feature_names_list = []\n",
    "    coefficients_list = []\n",
    "    for count, feature_i in enumerate(nonzero_indices):\n",
    "        feature_names_list.append(risk_score_model.featureNames[feature_i])\n",
    "        coefficients_list.append(int(risk_score_model.coefficients[feature_i]))\n",
    "    \n",
    "    print(\"feature names: \", feature_names_list)\n",
    "    print(\"coefficients: \", coefficients_list)\n",
    "    print(len(feature_names_list) == len(coefficients_list))\n",
    "\n",
    "def print_classification_metrics(risk_score_model, X, y):\n",
    "    start = time.time()\n",
    "    y_pred = risk_score_model.predict(X)\n",
    "    stop = time.time()\n",
    "    print(f\"Predict time: {stop-start} s\")\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    # Compute the precision\n",
    "    precision = precision_score(y, y_pred)\n",
    "    print(\"Precision: {:.3f}\".format(precision))\n",
    "    # Compute the recall or sensitivity\n",
    "    recall = recall_score(y, y_pred)\n",
    "    print(\"Recall: {:.3f}\".format(recall))\n",
    "    # Compute the F1 score\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    print(\"F1 score: {:.3f}\".format(f1))\n",
    "    # Compute the roc auc score\n",
    "    auc = roc_auc_score(y,y_pred)\n",
    "    print(\"AUC score: {:.3f}\".format(auc))\n",
    "    # Compute the log lossscore\n",
    "    loss = log_loss(y,y_pred)\n",
    "    print(\"Log loss: {:.3f}\".format(loss))\n",
    "\n",
    "    # Assume y and y_pred are the true and predicted labels for a binary classification problem\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    # Calculate TPR and TNR\n",
    "    tpr = tp / (tp + fn) # Sensitivity\n",
    "    tnr = tn / (tn + fp) # Specificity\n",
    "    # Calculate G-mean\n",
    "    gmean = np.sqrt(tpr * tnr)\n",
    "    print(\"G-mean: {:.3f}\".format(gmean))\n",
    "\n",
    "    print(\"Specificity: {:.3f}\".format(tnr))\n",
    "\n",
    "    # Print classification report and G-mean\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    print(\"{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\".format(accuracy,precision,recall,f1,auc,loss,tnr))\n",
    "\n",
    "    print(confusion_matrix(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85133a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(df, sparsity=5, parent_size=10, model_index=0):\n",
    "    ### DATAFRAME FORMATTING ###\n",
    "    print(df['label'].value_counts())\n",
    "    target_col = 'label'\n",
    "    df[target_col] = df[target_col].map({1: 1, 0: -1})  \n",
    "    print(df['label'].value_counts())\n",
    "    # Identify columns with boolean data type\n",
    "    bool_columns = df.select_dtypes(include=['bool']).columns\n",
    "    # Convert boolean columns to integer\n",
    "    df[bool_columns] = df[bool_columns].astype(int)\n",
    "    \n",
    "    ### SPLITTING THE DATA ###\n",
    "    X = df.drop(['label','account_id'], axis=1)\n",
    "    y = df['label']\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "    # Separate minority and majority classes\n",
    "    minority_class = df[df['label'] == 1]\n",
    "    majority_class = df[df['label'] == -1]\n",
    "    print(len(majority_class), len(minority_class))\n",
    "    # # Undersample majority class\n",
    "    # undersampled_majority_class = resample(majority_class, \n",
    "    #                                       replace=False, \n",
    "    #                                       n_samples=len(minority_class),\n",
    "    #                                       )\n",
    "\n",
    "    # # Combine minority class with undersampled majority class\n",
    "    # undersampled_data = pd.concat([minority_class, undersampled_majority_class])\n",
    "\n",
    "    # # Split the undersampled data into training, validation, and test sets\n",
    "    # X_undersampled = undersampled_data.drop(['label','account_id'], axis=1)\n",
    "    # y_undersampled = undersampled_data['label']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\n",
    "\n",
    "    # Print the number of examples in each set\n",
    "    print(\"Number of examples in the training set: \", len(X_train))\n",
    "    print(\"Number of examples in the validation set: \", len(X_val))\n",
    "    print(\"Number of examples in the test set: \", len(X_test))\n",
    "    # Calculate the desired number of samples for each class based on the proportion in the test dataset\n",
    "    desired_majority_samples = int(len(X_test) * 0.90)\n",
    "    desired_minority_samples = int(len(X_test) * 0.10)\n",
    "\n",
    "    print(\"desired\",desired_majority_samples, desired_minority_samples)\n",
    "    # Resample the majority class in the test dataset\n",
    "    resampled_majority_class = resample(majority_class,\n",
    "                                        replace=True,\n",
    "                                        n_samples=desired_majority_samples,\n",
    "                                        )\n",
    "\n",
    "    # Sample the minority class in the test dataset\n",
    "    sampled_minority_class = resample(minority_class,\n",
    "                                      replace=True,\n",
    "                                      n_samples=desired_minority_samples,\n",
    "                                      )\n",
    "\n",
    "    test_imbalanced = pd.concat([resampled_majority_class, sampled_minority_class])\n",
    "    X_test_imbalanced = test_imbalanced.drop(['label', 'account_id'], axis=1)\n",
    "    y_test_imbalanced= test_imbalanced['label']\n",
    "\n",
    "    print(\"Number of examples in the test imbalanced set: \", len(X_test_imbalanced))\n",
    "    # Print the number of examples in each class in the imbalanced test dataset\n",
    "    print(\"Number of examples in the imbalanced test dataset (label 0):\", len(y_test_imbalanced[y_test_imbalanced == 0]))\n",
    "    print(\"Number of examples in the imbalanced test dataset (label 1):\", len(y_test_imbalanced[y_test_imbalanced == 1]))\n",
    "      \n",
    "    ### CONVERT TO NUMPY ###\n",
    "    X_train = np.asarray(X_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    X_val = np.asarray(X_val)\n",
    "    y_val = np.asarray(y_val)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "    X_test_imbalanced = np.asarray(X_test_imbalanced)\n",
    "    y_test_imbalanced = np.asarray(y_test_imbalanced)\n",
    "\n",
    "    ### MODELLING ###\n",
    "#     sparsity = 5\n",
    "#     parent_size = 10\n",
    "    model = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity, parent_size = parent_size)\n",
    "    start_training_time = time.time()\n",
    "    model.optimize()\n",
    "    stop_training_time = time.time()\n",
    "    training_time = stop_training_time - start_training_time\n",
    "\n",
    "    multipliers, sparseDiversePool_beta0_integer, sparseDiversePool_betas_integer = model.get_models()\n",
    "    print(\"We generate {} risk score models from the sparse diverse pool\".format(len(multipliers)))\n",
    "    \n",
    "#     model_index = 0 # first model\n",
    "    multiplier = multipliers[model_index]\n",
    "    intercept = sparseDiversePool_beta0_integer[model_index]\n",
    "    coefficients = sparseDiversePool_betas_integer[model_index]\n",
    "    RiskScoreClassifier_m = RiskScoreClassifier(multiplier, intercept, coefficients)\n",
    "    ### RESULTS ###\n",
    "    ## Val\n",
    "    start_pred_val = time.time()\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    stop_pred_val = time.time()\n",
    "    pred_val_time = stop_pred_val - start_pred_val\n",
    "\n",
    "    accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "    precision_val = precision_score(y_val, y_pred_val)\n",
    "    recall_val = recall_score(y_val, y_pred_val)\n",
    "    f1_val = f1_score(y_val, y_pred_val)\n",
    "    auc_val = roc_auc_score(y_val,y_pred_val)\n",
    "    loss_val = log_loss(y_val,y_pred_val)\n",
    "    # Assume y_val and y_pred are the true and predicted labels for a binary classification problem\n",
    "    tn_val, fp_val, fn_val, tp_val = confusion_matrix(y_val, y_pred_val).ravel()\n",
    "    # Calculate TPR and TNR\n",
    "    tpr_val = tp_val / (tp_val + fn_val) # Sensitivity\n",
    "    tnr_val = tn_val / (tn_val + fp_val) # Specificity\n",
    "    # Calculate G-mean\n",
    "    gmean_val = np.sqrt(tpr_val * tnr_val) \n",
    "    ## Test\n",
    "    start_pred_test = time.time()\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    stop_pred_test = time.time()\n",
    "    pred_test_time = stop_pred_test - start_pred_test\n",
    "\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "    f1_test = f1_score(y_test, y_pred_test)\n",
    "    auc_test = roc_auc_score(y_test,y_pred_test)\n",
    "    loss_test = log_loss(y_test,y_pred_test)\n",
    "    # Assume y_test and y_pred are the true and predicted labels for a binary classification problem\n",
    "    tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "    # Calculate TPR and TNR\n",
    "    tpr_test = tp_test / (tp_test + fn_test) # Sensitivity\n",
    "    tnr_test = tn_test / (tn_test + fp_test) # Specificity\n",
    "    # Calculate G-mean\n",
    "    gmean_test = np.sqrt(tpr_test * tnr_test)\n",
    "    \n",
    "    ## test imbalanced\n",
    "    start_pred_test_imbalanced = time.time()\n",
    "    y_pred_test_imbalanced = model.predict(X_test_imbalanced)\n",
    "    stop_pred_test_imbalanced = time.time()\n",
    "    pred_test_imbalanced_time = stop_pred_test_imbalanced - start_pred_test_imbalanced\n",
    "\n",
    "    accuracy_test_imbalanced = accuracy_score(y_test_imbalanced, y_pred_test_imbalanced)\n",
    "    precision_test_imbalanced = precision_score(y_test_imbalanced, y_pred_test_imbalanced)\n",
    "    recall_test_imbalanced = recall_score(y_test_imbalanced, y_pred_test_imbalanced)\n",
    "    f1_test_imbalanced = f1_score(y_test_imbalanced, y_pred_test_imbalanced)\n",
    "    auc_test_imbalanced = roc_auc_score(y_test_imbalanced,y_pred_test_imbalanced)\n",
    "    loss_test_imbalanced = log_loss(y_test_imbalanced,y_pred_test_imbalanced)\n",
    "    # Assume y_test_imbalanced and y_pred are the true and predicted labels for a binary classification problem\n",
    "    tn_test_imbalanced, fp_test_imbalanced, fn_test_imbalanced, tp_test_imbalanced = confusion_matrix(y_test_imbalanced, y_pred_test_imbalanced).ravel()\n",
    "    # Calculate TPR and TNR\n",
    "    tpr_test_imbalanced = tp_test_imbalanced / (tp_test_imbalanced + fn_test_imbalanced) # Sensitivity\n",
    "    tnr_test_imbalanced = tn_test_imbalanced / (tn_test_imbalanced + fp_test_imbalanced) # Specificity\n",
    "    # Calculate G-mean\n",
    "    gmean_test_imbalanced = np.sqrt(tpr_test_imbalanced * tnr_test_imbalanced) \n",
    "\n",
    "    val_results = [accuracy_val, precision_val, recall_val, f1_val, auc_val, loss_val, tnr_val]\n",
    "    test_results = [accuracy_test, precision_test, recall_test, f1_test, auc_test, loss_test, tnr_test]\n",
    "    test_imbalanced_results = [accuracy_test_imbalanced, precision_test_imbalanced, recall_test_imbalanced, f1_test_imbalanced, auc_test_imbalanced, loss_test_imbalanced, tnr_test_imbalanced]\n",
    "\n",
    "    time_results = [training_time, pred_val_time, pred_test_time, pred_test_imbalanced_time]\n",
    "    return val_results, test_results, test_imbalanced_results, time_results\n",
    "    # return accuracy, precision, recall, f1, auc, loss, tnr   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16232362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iterations(model, X, y, iterations=5):\n",
    "    results = []\n",
    "    for i in range(iterations):\n",
    "        val_results, test_results, test_imbalanced_results, time_results = train_and_evaluate_model(model, X, y)\n",
    "        concatted_results = val_results + test_results + test_imbalanced_results + time_results\n",
    "    \n",
    "        results.append(concatted_results)\n",
    "\n",
    "    columns = ['Val Accuracy', 'Val Precision', 'Val Recall', 'Val F1', 'Val AUC', 'Val Loss', 'Val Specificity',\n",
    "               'Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1', 'Test AUC', 'Test Loss', 'Test Specificity',\n",
    "               'Test Imbalanced Accuracy', 'Test Imbalanced Precision', 'Test Imbalanced Recall', 'Test Imbalanced F1', 'Test Imbalanced AUC', 'Test Imbalanced Loss', 'Test Imbalanced Specificity',\n",
    "               'Training Time', 'Pred Val Time', 'Pred Test Time', 'Pred Test Imbalanced Time'\n",
    "               ]\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    print(df)\n",
    "    stats = df.describe().loc[['mean']]\n",
    "    # stats = df.describe()\n",
    "    print(stats)\n",
    "    return stats.to_csv(sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37490b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name ='data_f'\n",
    "dataset_file_path = '../dataset/' + dataset_name + '.csv'\n",
    "# train_data_file_path = \"../dataset/\"+ dataset_name + \"_train.csv\"\n",
    "# test_data_file_path = \"../dataset/\"+ dataset_name + \"_test.csv\"\n",
    "# val_data_file_path = \"../dataset/\"+ dataset_name + \"_val.csv\"\n",
    "# test_imbalanced_data_file_path = \"../dataset/\"+ dataset_name + \"_test_imbalanced.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3739e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'label'\n",
    "\n",
    "df = pd.read_csv(dataset_file_path)\n",
    "df[target_col] = df[target_col].map({1: 1, 0: -1})  \n",
    "# Identify columns with boolean data type\n",
    "bool_columns = df.select_dtypes(include=['bool']).columns\n",
    "# Convert boolean columns to integer\n",
    "df[bool_columns] = df[bool_columns].astype(int)\n",
    "\n",
    "\n",
    "# data = np.asarray(df)\n",
    "# X, y = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a97c6340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    3128\n",
      "Name: label, dtype: int64\n",
      "1.0    3128\n",
      "Name: label, dtype: int64\n",
      "0 3128\n",
      "Number of examples in the training set:  42926\n",
      "Number of examples in the validation set:  14309\n",
      "Number of examples in the test set:  14309\n",
      "desired 12878 1430\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "high <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v0/x7v75wv971ggrvzbs602023m0000gn/T/ipykernel_19224/2276829438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/v0/x7v75wv971ggrvzbs602023m0000gn/T/ipykernel_19224/2417101812.py\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(df, sparsity, parent_size, model_index)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"desired\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdesired_majority_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_minority_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Resample the majority class in the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     resampled_majority_class = resample(majority_class,\n\u001b[0m\u001b[1;32m     47\u001b[0m                                         \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                         \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesired_majority_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(replace, n_samples, random_state, stratify, *arrays)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_n_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: high <= 0"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_model(df,5,10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5d3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05988037",
   "metadata": {},
   "source": [
    "## Train Risk Score Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6772ac1",
   "metadata": {},
   "source": [
    "### Create RiskScoreOptimizer and Perform Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40955613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity = 5\n",
    "# parent_size = 10\n",
    "# RiskScoreOptimizer_m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity, parent_size = parent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# RiskScoreOptimizer_m.optimize()\n",
    "# print(\"Optimization takes {:.2f} seconds.\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92cdb18",
   "metadata": {},
   "source": [
    "## Get Risk Score Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multipliers, sparseDiversePool_beta0_integer, sparseDiversePool_betas_integer = RiskScoreOptimizer_m.get_models()\n",
    "# print(\"We generate {} risk score models from the sparse diverse pool\".format(len(multipliers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f482a4b",
   "metadata": {},
   "source": [
    "### Access the first risk score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_index = 0 # first model\n",
    "# multiplier = multipliers[model_index]\n",
    "# intercept = sparseDiversePool_beta0_integer[model_index]\n",
    "# coefficients = sparseDiversePool_betas_integer[model_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81d247",
   "metadata": {},
   "source": [
    "### Use the first risk score model to do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RiskScoreClassifier_m = RiskScoreClassifier(multiplier, intercept, coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_pred = RiskScoreClassifier_m.predict(X_test)\n",
    "# print(\"y_test are predicted to be {}\".format(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f66531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_pred_prob = RiskScoreClassifier_m.predict_prob(X_test)\n",
    "# print(\"The risk probabilities of having y_test to be +1 are {}\".format(y_test_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade6a55",
   "metadata": {},
   "source": [
    "### Print the first model card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_featureNames = list(train_df.columns[:-1])\n",
    "\n",
    "# RiskScoreClassifier_m.reset_featureNames(X_featureNames)\n",
    "# RiskScoreClassifier_m.print_model_card()\n",
    "\n",
    "# # get_calculation_table(RiskScoreClassifier_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73d9ae",
   "metadata": {},
   "source": [
    "### Print Top N Model Cards from the Pool and their performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de6a24c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# N = 5\n",
    "# num_models = min(N, len(multipliers))\n",
    "# print(\"Num models: \", num_models)\n",
    "# print(\"Number of multipliers: \", len(multipliers))\n",
    "# train_loss_list = []\n",
    "# test_auc_list = []\n",
    "# auc_list = []\n",
    "# for model_index in range(num_models):\n",
    "#     print(\"---------- Model {} ----------\".format(model_index+1))\n",
    "#     multiplier = multipliers[model_index]\n",
    "#     intercept = sparseDiversePool_beta0_integer[model_index]\n",
    "#     coefficients = sparseDiversePool_betas_integer[model_index]\n",
    "\n",
    "#     RiskScoreClassifier_m = RiskScoreClassifier(multiplier, intercept, coefficients)\n",
    "#     RiskScoreClassifier_m.reset_featureNames(X_featureNames)\n",
    "#     RiskScoreClassifier_m.print_model_card()\n",
    "\n",
    "#     train_loss = RiskScoreClassifier_m.compute_logisticLoss(X_train, y_train)\n",
    "#     train_acc, train_auc = RiskScoreClassifier_m.get_acc_and_auc(X_train, y_train)\n",
    "#     test_acc, test_auc = RiskScoreClassifier_m.get_acc_and_auc(X_test, y_test)\n",
    "\n",
    "#     print(\"The logistic loss on the training set is {}\".format(train_loss))\n",
    "#     print(\"The training accuracy and AUC are {:.3f}% and {:.3f}\".format(train_acc*100, train_auc))\n",
    "#     print(\"The test accuracy and AUC are are {:.3f}% and {:.3f}\\n\".format(test_acc*100, test_auc))\n",
    "\n",
    "#     print(\"### CLASSIFICATION REPORT - VAL ###\")\n",
    "#     print_classification_metrics(RiskScoreClassifier_m,X_val,y_val)\n",
    "    \n",
    "#     print(\"### CLASSIFICATION REPORT - TEST ###\")\n",
    "#     print_classification_metrics(RiskScoreClassifier_m,X_test,y_test)\n",
    "\n",
    "#     print(\"### CLASSIFICATION REPORT - TEST (IMBALANCED) ###\")\n",
    "#     print_classification_metrics(RiskScoreClassifier_m,X_test_imbalanced,y_test_imbalanced)\n",
    "    \n",
    "#     # TEST AUC\n",
    "#     y_pred = RiskScoreClassifier_m.predict(X_test)\n",
    "#     auc = roc_auc_score(y_test,y_pred)\n",
    "\n",
    "#     train_loss_list.append(round(train_loss,2))\n",
    "#     test_auc_list.append(round(test_auc,4))\n",
    "#     auc_list.append(round(auc,4))\n",
    "\n",
    "# avg_train_loss = sum(train_loss_list)/len(train_loss_list)\n",
    "# avg_test_auc = sum(test_auc_list)/len(test_auc_list)\n",
    "# avg_auc = sum(auc_list)/len(auc_list)\n",
    "\n",
    "# print(train_loss_list)\n",
    "# print(test_auc_list)\n",
    "\n",
    "# print(\"avg test auc: \", avg_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
